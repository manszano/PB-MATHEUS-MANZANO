<div>
  <img src="https://github.com/user-attachments/assets/9a2399c2-09f5-406d-8094-e1f2fa56d06e">
</div>

<br> 

<p align="center">
 <a href="#Sobre">Sobre</a> ‚Ä¢
 <a href="#ex">Exercic√≠o</a> ‚Ä¢
 <a href="#desafio">Desafio</a> ‚Ä¢
 <a href="#Certificados">Exerc√≠cios Docker</a>
</p>

---

<a id="Sobre"></a>
## üìù **Sobre**

### **Objetivo da Sprint**
Desenvolver um projeto em Python para manipula√ß√£o e an√°lise de dados do Prouni, integrando armazenamento em nuvem com AWS S3 via `boto3`.

---

### **Tarefas Realizadas**

- **Upload para o S3**
  - Carregado o arquivo original `ProuniRelatorioDadosAbertos2020.csv` para um bucket S3.
  - Utilizada a biblioteca `boto3` para configurar o cliente e realizar o upload do arquivo.

- **Manipula√ß√£o de Dados**
  - Carregado o arquivo do bucket S3 para um DataFrame com `pandas`.
  - Realizadas as seguintes opera√ß√µes no DataFrame:
    - **Filtro com m√∫ltiplos operadores l√≥gicos:** Sele√ß√£o de bolsas do tipo integral na regi√£o Sudeste.
    - **Fun√ß√µes de agrega√ß√£o:** Contagem de bolsas por curso e c√°lculo da m√©dia de idade dos benefici√°rios.
    - **Fun√ß√µes condicionais:** Cria√ß√£o da coluna `MAIOR_30`, indicando se o benefici√°rio possui 30 anos ou mais.
    - **Convers√£o de colunas:** Cria√ß√£o da coluna `SEXO_MASCULINO`, com valores booleanos para sexo masculino.
    - **Fun√ß√µes de manipula√ß√£o de data:** Convers√£o de `DATA_NASCIMENTO` para o ano de nascimento.
    - **Fun√ß√µes de string:** Transforma√ß√£o dos nomes dos cursos para letras mai√∫sculas.

- **Salvamento e Upload do Arquivo Processado**
  - O DataFrame manipulado foi salvo localmente como `ProuniRelatorioProcessado.csv`.
  - O arquivo processado foi enviado de volta ao bucket S3 para armazenamento final.

---

### **Anota√ß√µes Importantes**

#### _Aprendizados_

- Configura√ß√£o do cliente `boto3` para opera√ß√µes de upload e download em buckets S3.
- Manipula√ß√£o de dados com `pandas`, incluindo opera√ß√µes de filtro, agrega√ß√£o e transforma√ß√£o de strings e datas.
- Import√¢ncia de definir o encoding correto ao carregar arquivos CSV para evitar problemas de decodifica√ß√£o.

#### _Tecnologias Utilizadas_

- Python 3.12
- pandas
- boto3 (integra√ß√£o com AWS S3)

---

### **Desafios Enfrentados e Solu√ß√µes**

- **Problemas de Encoding**
  - **Desafio:** Erros de decodifica√ß√£o devido ao encoding incompat√≠vel (UTF-8) do CSV.
  - **Solu√ß√£o:** Utiliza√ß√£o do encoding `latin1`, compat√≠vel com caracteres acentuados em portugu√™s.

- **Manipula√ß√£o de Datas com Valores Nulos**
  - **Desafio:** Valores nulos ao calcular idades, devido a dados de nascimento inv√°lidos.
  - **Solu√ß√£o:** Uso do par√¢metro `errors='coerce'` para converter valores inv√°lidos em `NaT` (valores nulos).

- **Tratamento de Dados de Data e String**
  - **Desafio:** Necessidade de extra√ß√£o e transforma√ß√£o de colunas de data e texto.
  - **Solu√ß√£o:** Aplica√ß√£o de fun√ß√µes de `pandas` para manipula√ß√£o de strings e extra√ß√£o de dados de data, mantendo a consist√™ncia no DataFrame.

---

<a id="ex"></a>
## **Exercic√≠o: Configura√ß√£o de Bucket S3 para Hospedagem de Site Est√°tico**

Para configurar o Amazon S3 como um servidor de hospedagem de site est√°tico, foram seguidas as etapas abaixo. Esse processo permitiu armazenar e disponibilizar arquivos de forma p√∫blica, acess√≠veis atrav√©s de um endpoint de site gerado pelo S3.

### **Etapas Realizadas**

- **Etapa 1:** Criar um Bucket
  - Criado um bucket no S3 (_manzano.bucket.com_) para armazenar os arquivos do site.

- **Etapa 2:** Habilitar Hospedagem de Site Est√°tico
  - Nas configura√ß√µes do bucket, habilitada a op√ß√£o de "Hospedagem de site est√°tico", permitindo que o bucket sirva p√°ginas web.

- **Etapa 3:** Editar Configura√ß√µes de Bloqueio de Acesso P√∫blico
  - Modificadas as configura√ß√µes para desativar o bloqueio de acesso p√∫blico, habilitando o bucket para acesso p√∫blico.

- **Etapa 4:** Adicionar Pol√≠tica de Bucket para Acesso P√∫blico
  - Adicionada uma pol√≠tica de bucket para tornar todo o conte√∫do do bucket acess√≠vel publicamente. Essa pol√≠tica permite que qualquer usu√°rio acesse os arquivos armazenados.

- **Etapa 5:** Configurar Documento de √çndice
  - Definido um arquivo HTML: [index.html](https://github.com/manszano/PB-MATHEUS-MANZANO/blob/main/Sprint%205/exercicios/index.html), como o documento de √≠ndice, para que o site mostre essa p√°gina inicial automaticamente ao ser acessado.

- **Etapa 6:** Configurar Documento CSV
  - Configurado um documento csv: [nomes.csv](https://github.com/manszano/PB-MATHEUS-MANZANO/blob/main/Sprint%205/exercicios/nomes.csv) para pode ser efetuado do download.

- **Etapa 7:** Testar o Endpoint do Site
  - O endpoint gerado pelo S3 foi testado e √© o seguinte: _http://manzano.bucket.com.s3-website-us-east-1.amazonaws.com/_.

### **Considera√ß√µes Finais e evid√™ncias**

Essas configura√ß√µes permitiu que o bucket em quest√£o (_manzano.bucket.com_) sirva como uma hospedagem est√°tica simples.

![bucket-site-est√°tico](https://github.com/user-attachments/assets/64d291b9-79a7-410f-a3f5-911f00344e5e)
![site-est√°tico](https://github.com/user-attachments/assets/20650125-2d97-40c9-b16d-0ce0e8b5416f)

---

<a id="desafio"></a>
## üéØ **Desafio**

### Etapa 1: Upload do Arquivo para o Bucket S3

Para iniciar o desafio, foi criado um bucket S3 e realizado o upload do arquivo `ProuniRelatorioDadosAbertos2020.csv` com a biblioteca `boto3`.
Arquivo em quest√£o: [Arquivo.py](https://github.com/manszano/PB-MATHEUS-MANZANO/blob/main/Sprint%205/desafio/consulta.py)
```python
import boto3

# Configura√ß√£o do cliente S3
s3_client = boto3.client('s3',
    aws_access_key_id='XXXX',
    aws_secret_access_key='XXXX',
 aws_session_token = "XXXXXX",
    region_name='us-east-1'
)

# Nome do bucket e arquivo
bucket_name = "manzano-bucket-sprint5"
file_name = "ProuniRelatorioDadosAbertos2020.csv"

# Upload do arquivo
s3_client.upload_file(file_name, bucket_name, file_name)
```

### Etapa 2: Manipula√ß√£o dos Dados com Pandas

Ap√≥s o upload do arquivo, ele foi baixado do bucket S3, carregado em um DataFrame `pandas` e submetido a v√°rias opera√ß√µes de manipula√ß√£o.

```python
import pandas as pd

# Download do arquivo para leitura
    response = s3_client.get_object(Bucket=bucket_name, Key=object_name)
    file_content = response['Body'].read()

# Carregar o arquivo em um DataFrame
df = pd.read_csv('local_Prouni.csv', delimiter=';', encoding='latin1')

# Aplicar manipula√ß√£o de datas
df['DATA_NASCIMENTO'] = pd.to_datetime(df['DATA_NASCIMENTO'], errors='coerce')

if df['DATA_NASCIMENTO'].isnull().any():
  print("Data de Nascimento nula!")

# Aplicar filtros, fun√ß√µes de agrega√ß√£o e manipula√ß√µes
ano_atual = pd.Timestamp('today').year
df['IDADE'] = ano_atual - df['DATA_NASCIMENTO'].dt.year

df_filtrado = df[(df['TIPO_BOLSA'] == 'INTEGRAL') & (df['REGIAO_BENEFICIARIO'] == 'SUDESTE')]
contagem_bolsas = df_filtrado['NOME_CURSO_BOLSA'].value_counts()
media_idade = df_filtrado['IDADE'].mean()
df['MAIOR_30'] = df['IDADE'].apply(lambda x: 'Sim' if x >= 30 else 'N√£o')
df['SEXO_MASCULINO'] = df['SEXO_BENEFICIARIO'].apply(lambda x: True if x == 'M' else False)
df['ANO_NASCIMENTO'] = df['DATA_NASCIMENTO'].dt.year
df['NOME_CURSO_BOLSA'] = df['NOME_CURSO_BOLSA'].str.upper()

```

### Etapa 3: Salvar e Recarregar o Arquivo Processado no S3

O DataFrame manipulado foi salvo em um novo arquivo CSV e enviado novamente ao bucket S3.

```python
# Salvar o DataFrame processado em CSV e Upload no S3
processed_file_name = 'ProuniRelatorioProcessado.csv'
df.to_csv(processed_file_name, index=False, encoding='latin1')
s3_client.upload_file(processed_file_name, bucket_name, processed_file_name)
print(f"Arquivo '{processed_file_name}' salvo e carregado com sucesso no bucket '{bucket_name}'.")
```
### **Considera√ß√µes Finais e evid√™ncias**

Com isso foi poss√≠vel utilzar a api da AWS para fazer altera√ß√µes em um bucket do S3.
![bucket-desafio](https://github.com/user-attachments/assets/b50642a8-3a45-45ac-a3e1-0b14dfe26377)
![execu√ß√£o-aws](https://github.com/user-attachments/assets/b657dbee-6356-4620-b29e-4c05ad51f1ac)

---

<a id="Certificados"></a>
## üéì **Certificados**

### **Certificados Conquistados Durante a Sprint:**
![Cloud-Quest](https://github.com/user-attachments/assets/654e64f4-daa8-4613-9a4e-b8176e26716f)

*AWS Cloud Quest: Cloud Practitioner*


## üéØ **Conclus√£o da Sprint**

Nesta sprint, conclu√≠mos com sucesso os objetivos propostos, enfrentamos desafios importantes e adquirimos novos conhecimentos, gostei de aprender sobre AWS e de sua estrutura e estou empolgado para as pr√≥ximas Sprints!!

![bottom](https://github.com/user-attachments/assets/a06b7240-a4be-45d7-86e7-9427136b3891)
