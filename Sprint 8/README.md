![header-image](#)

<p align="center">
 <a href="#Sobre">Sobre</a> ‚Ä¢
 <a href="#exercicio">Exerc√≠cio</a> ‚Ä¢
 <a href="#desafio">Desafio</a>
</p>

---

<a id="Sobre"></a>
## üìù **Sobre**

### **Objetivo do Projeto**
Este projeto tem como objetivo aplicar t√©cnicas de manipula√ß√£o de dados e processamento distribu√≠do utilizando Apache Spark. Os exerc√≠cios consistem em opera√ß√µes b√°sicas com listas e manipula√ß√£o de DataFrames, enquanto o desafio √© implementado no AWS Glue para transforma√ß√£o e organiza√ß√£o de dados em um Data Lake.

---

### **Tarefas Realizadas**
- **Exerc√≠cio - Warm Up com Python**
  - Manipula√ß√£o de listas.
  - Gera√ß√£o de arquivos CSV.
- **Exerc√≠cio - Manipula√ß√£o com Spark**
  - Opera√ß√µes com DataFrames no PySpark.
  - Integra√ß√£o de bibliotecas auxiliares.
- **Desafio - Processamento com Glue**
  - Transforma√ß√£o de dados da Raw Zone para a Trusted Zone.
  - Organiza√ß√£o de dados no formato Parquet, particionados e integrados ao Glue Data Catalog.

---

<a id="exercicio"></a>
## **Exerc√≠cio**

### **Objetivo**
Realizar manipula√ß√£o de listas e DataFrames, criando solu√ß√µes b√°sicas em Python e Apache Spark.

---

### **Exerc√≠cio 1: Manipula√ß√£o de Listas**
```python
import random

# Declara√ß√£o da lista com 250 n√∫meros inteiros aleat√≥rios
lista_numeros = random.sample(range(1, 1000), 250)

# Revertendo a lista
lista_numeros.reverse()

# Imprimindo o resultado
print(lista_numeros)
```

---

### **Exerc√≠cio 2: Manipula√ß√£o de Listas com Arquivos**
```python
# Lista com 20 nomes de animais
animais = ["Cachorro", "Gato", "Papagaio", "Pato", "Tartaruga", "Arara", "Le√£o", 
           "Tigre", "Elefante", "Cavalo", "Porco", "Macaco", "Lobo", "Veado", 
           "Ganso", "Cobra", "Raposa", "On√ßa", "Bicho-pregui√ßa", "Capivara"]

# Ordenando e imprimindo os animais
animais.sort()
[print(animal) for animal in animais]

# Salvando em arquivo CSV
with open("animais.csv", "w") as file:
    for animal in animais:
        file.write(f"{animal}\n")
```

---

### **Exerc√≠cio 3: Gera√ß√£o de Dataset de Nomes**
```python
import random
import names

# Defini√ß√µes iniciais
random.seed(40)
qtd_nomes_unicos = 3008
qtd_nomes_aleatorios = 10000000

# Gerando nomes √∫nicos
nomes_unicos = [names.get_full_name() for _ in range(qtd_nomes_unicos)]

# Gerando 10 milh√µes de nomes aleat√≥rios
dados = [random.choice(nomes_unicos) for _ in range(qtd_nomes_aleatorios)]

# Salvando em arquivo
with open("nomes_aleatorios.txt", "w") as file:
    file.write("\n".join(dados))
```

---

### **Exerc√≠cio 4: Manipula√ß√£o com PySpark**
#### **Etapa 1: Configura√ß√£o Inicial**
```python
from pyspark.sql import SparkSession

# Configurando a Spark Session
spark = SparkSession.builder.master("local[*]").appName("Exercicio Intro").getOrCreate()
df_nomes = spark.read.csv("nomes_aleatorios.txt", header=False)

# Visualizando os dados
df_nomes.show(5)
```

#### **Etapa 2: Renomear e Verificar Schema**
```python
# Renomeando a coluna
df_nomes = df_nomes.withColumnRenamed("_c0", "Nomes")

# Verificando o esquema
df_nomes.printSchema()
df_nomes.show(10)
```

#### **Etapas 3-5: Adicionando Colunas**
```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
import random

# Fun√ß√µes UDF
escolaridade_udf = udf(lambda: random.choice(["Fundamental", "Medio", "Superior"]), StringType())
pais_udf = udf(lambda: random.choice(["Brasil", "Argentina", "Chile", "Col√¥mbia", "Uruguai"]), StringType())
ano_nascimento_udf = udf(lambda: str(random.randint(1945, 2010)), StringType())

# Adicionando colunas
df_nomes = df_nomes.withColumn("Escolaridade", escolaridade_udf())
df_nomes = df_nomes.withColumn("Pais", pais_udf())
df_nomes = df_nomes.withColumn("AnoNascimento", ano_nascimento_udf())

df_nomes.show(10)
```

---

<a id="desafio"></a>
## üéØ **Desafio**

### **Objetivo**
Transformar dados da camada RAW para a camada TRUSTED em um Data Lake utilizando o AWS Glue. 

---

### **C√≥digo do Job 1: CSV para Trusted Zone**
```python
import sys
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# Lendo argumentos
raw_path = sys.argv[1]
trusted_path = sys.argv[2]

# Lendo CSV
df = spark.read.csv(raw_path, header=True)

# Salvando como Parquet
df.write.mode("overwrite").parquet(trusted_path)
```

---

### **C√≥digo do Job 2: API TMDB para Trusted Zone**
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import current_date

spark = SparkSession.builder.getOrCreate()

# Lendo argumentos
raw_path = sys.argv[1]
trusted_path = sys.argv[2]

# Lendo JSON
df = spark.read.json(raw_path)

# Particionando e salvando
df = df.withColumn("partition_date", current_date())
df.write.mode("overwrite").partitionBy("partition_date").parquet(trusted_path)
```

---

## üéØ **Conclus√£o**
Os exerc√≠cios e o desafio mostraram como manipular dados de maneira eficiente em Python e Spark. A implementa√ß√£o no AWS Glue permite escalar o processamento em um ambiente distribu√≠do.

![footer-image](#)
